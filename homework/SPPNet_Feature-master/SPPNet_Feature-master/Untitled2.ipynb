{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0544e955",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-63a4ebdf62b2>, line 232)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-63a4ebdf62b2>\"\u001b[1;36m, line \u001b[1;32m232\u001b[0m\n\u001b[1;33m    print int(conv5.get_shape()[0]), int(conv5.get_shape()[1]), int(conv5.get_shape()[2])\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import scipy.io\n",
    "import PIL\n",
    "from numpy import *\n",
    "from pylab import *\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE  = 0.1\n",
    "VALIDATION_SIZE = 0\n",
    "TRAINING_ITERATIONS = 50000\n",
    "WEIGHT_DECAY = 0.00005\n",
    "\n",
    "net_data = load(\"bvlc_alexnet.npy\").item()\n",
    "\n",
    "out_pool_size = [8, 6, 4]\n",
    "hidden_dim = 0\n",
    "for item in out_pool_size:\n",
    "  hidden_dim = hidden_dim + item * item\n",
    "  \n",
    "data_folder = './102flowers'\n",
    "labels = scipy.io.loadmat('imagelabels.mat')\n",
    "setid = scipy.io.loadmat('setid.mat')\n",
    "\n",
    "labels = labels['labels'][0] - 1\n",
    "trnid = np.array(setid['tstid'][0]) - 1\n",
    "tstid = np.array(setid['trnid'][0]) - 1\n",
    "valid = np.array(setid['valid'][0]) - 1\n",
    "\n",
    "num_classes = 102\n",
    "data_dir = list()\n",
    "for img in os.listdir(data_folder):\n",
    "  data_dir.append(os.path.join(data_folder, img))\n",
    "\n",
    "data_dir.sort()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Ultils\n",
    "def print_activations(t):\n",
    "  print(t.op.name, ' ', t.get_shape().as_list())\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "  num_labels = labels_dense.shape[0]\n",
    "  index_offset = np.arange(num_labels) * num_classes\n",
    "  labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n",
    "\n",
    "def read_images_from_disk(input_queue):\n",
    "  label = input_queue[1]\n",
    "  file_contents = tf.read_file(input_queue[0])\n",
    "  example = tf.image.decode_jpeg(file_contents, channels=3)\n",
    "  # example = tf.cast(example, tf.float32 )\n",
    "  return example, label\n",
    "\n",
    "def weight_variable(shape, name):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.01, name=name)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "  initial = tf.constant(0.1, shape=shape, name=name)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w, padding = \"VALID\", group = 1):\n",
    "  '''From https://github.com/ethereon/caffe-tensorflow\n",
    "  '''\n",
    "  c_i = input.get_shape()[-1]\n",
    "  assert c_i % group == 0\n",
    "  assert c_o % group == 0\n",
    "  convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "  if group == 1:\n",
    "    conv = convolve(input, kernel)\n",
    "  else:\n",
    "    input_groups = tf.split(axis=3, num_or_size_splits=group, value=input)\n",
    "    kernel_groups = tf.split(axis=3, num_or_size_splits=group, value=kernel)\n",
    "    output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "    conv = tf.concat(axis=3, values=output_groups)\n",
    "  return tf.reshape(tf.nn.bias_add(conv, biases), [-1] + conv.get_shape().as_list()[1:])\n",
    "\n",
    "def conv2d(x, W, stride_h, stride_w, padding='SAME'):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, stride_h, stride_w, 1], padding=padding)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_4x4(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')\n",
    "\n",
    "# Spatial Pyramid Pooling block\n",
    "# https://arxiv.org/abs/1406.4729\n",
    "def spatial_pyramid_pool(previous_conv, num_sample, previous_conv_size, out_pool_size):\n",
    "  \"\"\"\n",
    "  previous_conv: a tensor vector of previous convolution layer\n",
    "  num_sample: an int number of image in the batch\n",
    "  previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n",
    "  out_pool_size: a int vector of expected output size of max pooling layer\n",
    "  \n",
    "  returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n",
    "  \"\"\"\n",
    "  for i in range(len(out_pool_size)):\n",
    "    h_strd = h_size = math.ceil(float(previous_conv_size[0]) / out_pool_size[i])\n",
    "    w_strd = w_size = math.ceil(float(previous_conv_size[1]) / out_pool_size[i])\n",
    "    pad_h = int(out_pool_size[i] * h_size - previous_conv_size[0])\n",
    "    pad_w = int(out_pool_size[i] * w_size - previous_conv_size[1])\n",
    "    new_previous_conv = tf.pad(previous_conv, tf.constant([[0, 0], [0, pad_h], [0, pad_w], [0, 0]]))\n",
    "    max_pool = tf.nn.max_pool(new_previous_conv,\n",
    "                   ksize=[1,h_size, h_size, 1],\n",
    "                   strides=[1,h_strd, w_strd,1],\n",
    "                   padding='SAME')\n",
    "    if (i == 0):\n",
    "      spp = tf.reshape(max_pool, [num_sample, -1])\n",
    "    else:\n",
    "      spp = tf.concat(axis=1, values=[spp, tf.reshape(max_pool, [num_sample, -1])])\n",
    "  \n",
    "  return spp\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Modeling\n",
    "size_cluster = defaultdict(list)\n",
    "for tid in trnid:\n",
    "  img = Image.open(data_dir[tid])\n",
    "  key = (img.size[0] - img.size[0] % 10, img.size[1] - img.size[1] % 10)\n",
    "  size_cluster[key].append(tid)\n",
    "  \n",
    "size_cluster_keys = size_cluster.keys()\n",
    "\n",
    "train_accuracies = []\n",
    "train_cost = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "batch_size = 20\n",
    "print('Training ...')\n",
    "\n",
    "# Training block\n",
    "# 1. Combime all iamges have the same size to a batch.\n",
    "# 2. Then, train parameters in a batch\n",
    "# 3. Transfer trained parameters to another batch\n",
    "it = 0\n",
    "while it < TRAINING_ITERATIONS:\n",
    "  graph = tf.Graph()\n",
    "  with graph.as_default():\n",
    "    y_train = labels[size_cluster[size_cluster_keys[it%len(size_cluster_keys)]]]\n",
    "    if len(y_train) < 50:\n",
    "      batch_size = len(y_train)\n",
    "\n",
    "    y_train = dense_to_one_hot(y_train, num_classes)\n",
    "    x_train = [data_dir[i] for i in size_cluster[size_cluster_keys[it%len(size_cluster_keys)]]]\n",
    "\n",
    "    input_queue_train = tf.train.slice_input_producer([x_train, y_train],\n",
    "                            num_epochs=None,\n",
    "                            shuffle=True)\n",
    "\n",
    "    x_train, y_train = read_images_from_disk(input_queue_train)\n",
    "\n",
    "    print(size_cluster_keys[it%len(size_cluster_keys)])\n",
    "    x_train = tf.image.resize_images(x_train,\n",
    "                     [size_cluster_keys[it%len(size_cluster_keys)][1]/2,\n",
    "                     size_cluster_keys[it%len(size_cluster_keys)][0]/2],\n",
    "                     method=1, align_corners=False)\n",
    "\n",
    "    x_train, y_train = tf.train.batch([x_train, y_train], batch_size = batch_size)\n",
    "\n",
    "    x = tf.placeholder('float', shape = x_train.get_shape())\n",
    "    y_ = tf.placeholder('float', shape = [None, num_classes])\n",
    "\n",
    "    conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "    conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "    conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "    conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "    conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "    conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "    conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "    conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "    conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "    conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "    fc6W = weight_variable([hidden_dim * 256, 4096], 'fc6W')\n",
    "    fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "    fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "    fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "    fc8W = weight_variable([4096, num_classes], 'W_fc8')\n",
    "    fc8b = bias_variable([num_classes], 'b_fc8')\n",
    "    keep_prob = tf.placeholder('float')\n",
    "\n",
    "\n",
    "    def model(x):\n",
    "      # conv1\n",
    "      conv1 = tf.nn.relu(conv(x, conv1W, conv1b, 11, 11, 96, 4, 4, padding=\"SAME\", group=1))\n",
    "      # lrn1\n",
    "      # lrn(2, 2e-05, 0.75, name='norm1')\n",
    "      lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                            depth_radius=5,\n",
    "                            alpha=0.0001,\n",
    "                            beta=0.75,\n",
    "                            bias=1.0)\n",
    "      # maxpool1\n",
    "      maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "      # conv2\n",
    "      conv2 = tf.nn.relu(conv(maxpool1, conv2W, conv2b, 5, 5, 256, 1, 1, padding=\"SAME\", group=2))\n",
    "      # lrn2\n",
    "      # lrn(2, 2e-05, 0.75, name='norm2')\n",
    "      lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                            depth_radius=5,\n",
    "                            alpha=0.0001,\n",
    "                            beta=0.75,\n",
    "                            bias=1.0)\n",
    "      # maxpool2\n",
    "      maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "      # conv3\n",
    "      conv3 = tf.nn.relu(conv(maxpool2, conv3W, conv3b, 3, 3, 384, 1, 1, padding=\"SAME\", group=1))\n",
    "      # conv4\n",
    "      conv4 = tf.nn.relu(conv(conv3, conv4W, conv4b, 3, 3, 384, 1, 1, padding=\"SAME\", group=2))\n",
    "      # conv5\n",
    "      conv5 = tf.nn.relu(conv(conv4, conv5W, conv5b, 3, 3, 256, 1, 1, padding=\"SAME\", group=2))\n",
    "      print int(conv5.get_shape()[0]), int(conv5.get_shape()[1]), int(conv5.get_shape()[2])\n",
    "      maxpool5 = spatial_pyramid_pool(conv5,\n",
    "                      int(conv5.get_shape()[0]),\n",
    "                       [int(conv5.get_shape()[1]), int(conv5.get_shape()[2])],\n",
    "                       out_pool_size)\n",
    "      # fc6\n",
    "      fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n",
    "      fc6_drop = tf.nn.dropout(fc6, keep_prob)\n",
    "      # fc7\n",
    "      fc7 = tf.nn.relu_layer(fc6_drop, fc7W, fc7b)\n",
    "      fc7_drop = tf.nn.dropout(fc7, keep_prob)\n",
    "      # fc8\n",
    "      fc8 = tf.nn.xw_plus_b(fc7_drop, fc8W, fc8b)\n",
    "      return fc8\n",
    "    \n",
    "    logits = model(x)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "    regularizers = tf.nn.l2_loss(conv1W) + tf.nn.l2_loss(conv1b) + \\\n",
    "             tf.nn.l2_loss(conv2W) + tf.nn.l2_loss(conv2b) + \\\n",
    "             tf.nn.l2_loss(conv3W) + tf.nn.l2_loss(conv3b) + \\\n",
    "             tf.nn.l2_loss(conv4W) + tf.nn.l2_loss(conv4b) + \\\n",
    "             tf.nn.l2_loss(conv5W) + tf.nn.l2_loss(conv5b) + \\\n",
    "             tf.nn.l2_loss(fc6W) + tf.nn.l2_loss(fc6b) + \\\n",
    "             tf.nn.l2_loss(fc7W) + tf.nn.l2_loss(fc7b) + \\\n",
    "             tf.nn.l2_loss(fc8W) + tf.nn.l2_loss(fc8b)\n",
    "\n",
    "    loss = tf.reduce_mean(cross_entropy + WEIGHT_DECAY * regularizers)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "    # optimisation loss function\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE, global_step, 1000, 0.9, staircase=True)\n",
    "    train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # evaluation\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    predict = tf.argmax(logits, 1)\n",
    "    saver = tf.train.Saver({v.op.name: v for v in [conv1W, conv1b,\n",
    "                             conv2W, conv2b,\n",
    "                             conv3W, conv3b,\n",
    "                             conv4W, conv4b,\n",
    "                             conv5W, conv5b,\n",
    "                             fc6W, fc6b,\n",
    "                             fc7W, fc7b,\n",
    "                             fc8W, fc8b]})\n",
    "\n",
    "\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    if os.path.exists('./alex_model_spp.ckpt'):\n",
    "      saver.restore(sess, './alex_model_spp.ckpt')\n",
    "\n",
    "    cnt_tmp = 0\n",
    "    xtrain, ytrain = sess.run([x_train, y_train])\n",
    "    for i in range(10):\n",
    "      it = it + 1\n",
    "      _, train_accuracy, cost = sess.run([train_step, accuracy, cross_entropy], \n",
    "                      feed_dict = {x: xtrain,\n",
    "                             y_: ytrain, \n",
    "                             keep_prob: 1.0})\n",
    "      \n",
    "      print('training_accuracy => %.4f, cost value => %.4f for step %d'\n",
    "          %(train_accuracy, cost, it))\n",
    "\n",
    "      if (train_accuracy > 0.95):\n",
    "        cnt_tmp = cnt_tmp + 1\n",
    "\n",
    "      if (cnt_tmp > 10):\n",
    "        break\n",
    "\n",
    "      train_accuracies.append(train_accuracy)\n",
    "      x_range.append(it)\n",
    "      train_cost.append(cost)\n",
    "\n",
    "    saver.save(sess, './alex_model_spp.ckpt')\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "  sess.close()\n",
    "  del sess\n",
    "  \n",
    "# Plot accuracy and loss curve\n",
    "plt.plot(x_range, train_cost,'-b')\n",
    "plt.ylabel('spp_cost')\n",
    "plt.xlabel('step')\n",
    "plt.savefig('spp_cost.png')\n",
    "plt.close()\n",
    "plt.plot(x_range, train_accuracies,'-b')\n",
    "plt.ylabel('spp_accuracies')\n",
    "plt.ylim(ymax = 1.1)\n",
    "plt.xlabel('step')\n",
    "plt.savefig('spp_accuracy.png')\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Testing block\n",
    "# 1. Gather all images have the same size into a batch\n",
    "# 2. Feed to Alexnet_SPP to predict the expected labels\n",
    "it = 0\n",
    "result = list()\n",
    "f = open('result_spp.txt', 'w')\n",
    "while it < len(tstid):\n",
    "  if (it % 10 == 0):\n",
    "    print(it)\n",
    "  graph = tf.Graph()\n",
    "  with graph.as_default():\n",
    "    # with tf.device('/cpu:0'):\n",
    "    img = Image.open(data_dir[tstid[it]])\n",
    "    filename_queue = tf.train.string_input_producer([data_dir[tstid[it]]])\n",
    "    reader = tf.WholeFileReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    my_img = tf.image.decode_jpeg(value, channels = 3)\n",
    "    # my_img = tf.cast(my_img, tf.float32)\n",
    "    my_img = tf.image.resize_images(my_img,\n",
    "                    [img.size[1] / 2,\n",
    "                    img.size[0] / 2],\n",
    "                    method = 1,\n",
    "                    align_corners = False)\n",
    "\n",
    "    my_img = tf.expand_dims(my_img, 0)\n",
    "\n",
    "    x = tf.placeholder('float', shape=my_img.get_shape())\n",
    "    print(my_img.get_shape())\n",
    "    conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "    conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "    conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "    conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "    conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "    conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "    conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "    conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "    conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "    conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "    fc6W = weight_variable([hidden_dim * 256, 4096], 'fc6W')\n",
    "    fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "    fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "    fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "    fc8W = weight_variable([4096, num_classes], 'W_fc8')\n",
    "    fc8b = bias_variable([num_classes], 'b_fc8')\n",
    "    keep_prob = tf.placeholder('float')\n",
    "\n",
    "\n",
    "    def model(x):\n",
    "      # conv1\n",
    "      conv1 = tf.nn.relu(conv(x, conv1W, conv1b, 11, 11, 96, 4, 4, padding=\"SAME\", group=1))\n",
    "      # lrn1\n",
    "      # lrn(2, 2e-05, 0.75, name='norm1')\n",
    "      lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                            depth_radius=5,\n",
    "                            alpha=0.0001,\n",
    "                            beta=0.75,\n",
    "                            bias=1.0)\n",
    "      # maxpool1\n",
    "      maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "      # conv2\n",
    "      conv2 = tf.nn.relu(conv(maxpool1, conv2W, conv2b, 5, 5, 256, 1, 1, padding=\"SAME\", group=2))\n",
    "      # lrn2\n",
    "      # lrn(2, 2e-05, 0.75, name='norm2')\n",
    "      lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                            depth_radius=5,\n",
    "                            alpha=0.0001,\n",
    "                            beta=0.75,\n",
    "                            bias=1.0)\n",
    "      # maxpool2\n",
    "      maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "      # conv3\n",
    "      conv3 = tf.nn.relu(conv(maxpool2, conv3W, conv3b, 3, 3, 384, 1, 1, padding=\"SAME\", group=1))\n",
    "      # conv4\n",
    "      conv4 = tf.nn.relu(conv(conv3, conv4W, conv4b, 3, 3, 384, 1, 1, padding=\"SAME\", group=2))\n",
    "      # conv5\n",
    "      conv5 = tf.nn.relu(conv(conv4, conv5W, conv5b, 3, 3, 256, 1, 1, padding=\"SAME\", group=2))\n",
    "      maxpool5 = spatial_pyramid_pool(conv5,\n",
    "                      int(conv5.get_shape()[0]),\n",
    "                       [int(conv5.get_shape()[1]), int(conv5.get_shape()[2])],\n",
    "                       out_pool_size)\n",
    "      # fc6\n",
    "      fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n",
    "      fc6_drop = tf.nn.dropout(fc6, keep_prob)\n",
    "      # fc7\n",
    "      fc7 = tf.nn.relu_layer(fc6_drop, fc7W, fc7b)\n",
    "      fc7_drop = tf.nn.dropout(fc7, keep_prob)\n",
    "      # fc8\n",
    "      fc8 = tf.nn.xw_plus_b(fc7_drop, fc8W, fc8b)\n",
    "      prob = tf.nn.softmax(fc8)\n",
    "      return prob\n",
    "\n",
    "    logits = model(x)\n",
    "    predict = tf.argmax(logits, 1)\n",
    "    saver = tf.train.Saver({v.op.name: v for v in [conv1W, conv1b,\n",
    "                             conv2W, conv2b,\n",
    "                             conv3W, conv3b,\n",
    "                             conv4W, conv4b,\n",
    "                             conv5W, conv5b,\n",
    "                             fc6W, fc6b,\n",
    "                             fc7W, fc7b,\n",
    "                             fc8W, fc8b]})\n",
    "\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    saver.restore(sess, './alex_model_spp.ckpt')\n",
    "    image = sess.run(my_img)\n",
    "    predict = predict.eval(feed_dict={x: image, keep_prob: 1.0})\n",
    "    result.append(predict[0])\n",
    "    f.write(data_dir[tstid[it]] + '\\t' + str(predict[0]) + '\\t' + str(labels[tstid[it]]))\n",
    "    f.write('\\n')\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "  sess.close()\n",
    "  del sess\n",
    "  it = it + 1\n",
    "\n",
    "print('Test accuracy: %f' %(sum(np.array(result) == np.array(labels[tstid])).astype('float')/len(tstid)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5f0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
